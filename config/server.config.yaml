server:
    web_port: 9190
    anthropic_port: 9170
    ollama_port: 11434
    lmstudio_port: 1234
    host: 0.0.0.0
    read_timeout: 60
    write_timeout: 60
model:
    paths:
        - ./models
        - ~/.cache/huggingface/hub
    path_configs:
        - path: /tmp/test_models
          name: 测试目录
          description: ""
    auto_scan: true
    scan_interval: 0
llamacpp:
    paths:
        - path: ./llama.cpp
          name: Default
          description: ""
        - path: /home/user/workspace/llama.cpp/build-rocm/bin
          name: Rocm后端
          description: ""
download:
    directory: ./downloads
    max_concurrent: 4
    chunk_size: 1048576
    retry_count: 3
    timeout: 300
security:
    api_key_enabled: false
    api_key: ""
    cors_enabled: true
    allowed_origins:
        - '*'
compatibility:
    ollama:
        enabled: true
        port: 11434
    lmstudio:
        enabled: false
        port: 1234
log:
    level: info
    format: json
    output: both
    directory: ./logs
    max_size: 100
    max_backups: 3
    max_age: 7
    compress: true
mode: standalone
master:
    enabled: false
    client_config_dir: ""
    network_scan:
        enabled: false
        subnets: []
        port_range: ""
        timeout: 0
        auto_discover: false
        interval: 0
    scheduler:
        strategy: ""
        max_queue_size: 0
        task_timeout: 0
        retry_on_failure: false
        max_retries: 0
    log_aggregation:
        enabled: false
        max_buffer_size: 0
        flush_interval: 0
client:
    enabled: false
    master_address: ""
    client_info:
        id: ""
        name: ""
        tags: []
        metadata: {}
    heartbeat:
        interval: 0
        timeout: 0
    conda_env:
        enabled: false
        conda_path: ""
        environments: {}
