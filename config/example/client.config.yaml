# Shepherd Client 模式示例配置
# Client 模式：作为工作节点连接到 Master 服务器
# 适合作为 GPU 工作节点加入集群

# 运行模式
mode: client

# 节点配置
node:
  # 节点ID，auto 表示自动生成
  id: auto
  # 节点名称，空表示使用主机名
  name: ""
  # 节点角色：client
  role: client
  
  # Master 角色配置（Client 模式下禁用）
  master_role:
    enabled: false
    port: 9190
    api_key: ""
    ssl:
      enabled: false
      cert_path: ""
      key_path: ""
  
  # Client 角色配置
  client_role:
    enabled: true
    master_address: http://192.168.1.100:9190  # Master 服务器地址
    register_retry: 3              # 注册失败重试次数
    heartbeat_interval: 5          # 心跳间隔（秒）
    heartbeat_timeout: 15          # 心跳超时（秒）
  
  # 资源监控配置
  resources:
    monitor_interval: 5            # 监控间隔（秒）
    report_gpu: true               # 报告 GPU 信息
    report_temperature: true       # 报告温度信息
    gpu_backend: auto              # GPU 后端
  
  # 命令执行器配置
  executor:
    max_concurrent: 4              # 最大并发任务数
    task_timeout: 3600             # 任务超时（秒）
    allow_remote_stop: true        # 允许 Master 远程停止任务
    allowed_commands:              # 允许执行的命令
      - load_model
      - unload_model
      - run_llamacpp
      - stop_process
      - scan_models
      - collect_logs

# 服务器配置
server:
  # Client 端口，应与 Master 不同
  web_port: 9191
  anthropic_port: 9170
  ollama_port: 11434
  lmstudio_port: 1234
  host: 0.0.0.0
  read_timeout: 60
  write_timeout: 60

# 模型配置
model:
  paths:
    - ./models
    - ~/.cache/huggingface/hub
  auto_scan: true
  scan_interval: 0

# llama.cpp 配置
llamacpp:
  paths:
    - path: ./llama.cpp
      name: Default
      description: 默认 llama.cpp 路径

# 下载配置
download:
  directory: ./downloads
  max_concurrent: 4
  chunk_size: 1048576
  retry_count: 3
  timeout: 300

# 模型仓库配置
model_repo:
  endpoint: hf-mirror.com
  token: ""
  timeout: 30

# 安全配置
security:
  api_key_enabled: false
  api_key: ""
  cors_enabled: true
  allowed_origins:
    - "*"

# 兼容性配置
compatibility:
  ollama:
    enabled: true
    port: 11434
  lmstudio:
    enabled: false
    port: 1234

# 日志配置
log:
  level: info
  format: json
  output: both
  directory: ./logs
  max_size: 100
  max_backups: 3
  max_age: 7
  compress: true

# 存储配置
storage:
  type: sqlite
  sqlite:
    path: ./data/shepherd.db
    enable_wal: true
    pragmas:
      cache_size: "-64000"
      synchronous: "NORMAL"

# Master 配置（Client 模式下禁用）
master:
  enabled: false
  client_config_dir: ./config/clients
  network_scan:
    enabled: false
    subnets: []
    port_range: ""
    timeout: 0
    auto_discover: false
    interval: 0
  scheduler:
    strategy: ""
    max_queue_size: 0
    task_timeout: 0
    retry_on_failure: false
    max_retries: 0
  log_aggregation:
    enabled: false
    max_buffer_size: 0
    flush_interval: 0

# Client 配置（旧格式兼容）
client:
  enabled: true
  master_address: http://192.168.1.100:9190
  client_info:
    id: ""
    name: ""
    tags:
      - gpu
    metadata:
      os: linux
      arch: amd64
  heartbeat:
    interval: 30
    timeout: 90
  conda_env:
    enabled: false
    conda_path: ""
    environments: {}
  register_retry: 3
  heartbeat_interval: 5
  heartbeat_timeout: 15
