server:
    web_port: 9190
    anthropic_port: 9170
    ollama_port: 11434
    lmstudio_port: 1234
    host: 0.0.0.0
    read_timeout: 60
    write_timeout: 60
model:
    paths: []
    path_configs: []
    auto_scan: false
    scan_interval: 0
llamacpp:
    paths:
        - path: /home/user/workspace/Shepherd/internal/api/storage/llama.cpp
          name: Default
          description: ""
download:
    directory: /home/user/workspace/Shepherd/internal/api/storage/downloads
    max_concurrent: 4
    chunk_size: 1048576
    retry_count: 3
    timeout: 300
model_repo:
    endpoint: huggingface.co
    token: ""
    timeout: 30
security:
    api_key_enabled: false
    api_key: ""
    cors_enabled: true
    allowed_origins:
        - '*'
compatibility:
    ollama:
        enabled: true
        port: 11434
    lmstudio:
        enabled: false
        port: 1234
log:
    level: info
    format: json
    output: both
    directory: /home/user/workspace/Shepherd/internal/api/storage/logs
    max_size: 100
    max_backups: 3
    max_age: 7
    compress: true
storage:
    type: sqlite
    sqlite:
        path: /tmp/test.db
        pragmas: {}
        enable_wal: false
    postgresql: null
mode: standalone
master:
    enabled: false
    client_config_dir: /home/user/workspace/Shepherd/internal/api/storage/config/clients
    network_scan:
        enabled: false
        subnets:
            - 192.168.1.0/24
            - 10.0.0.0/8
        port_range: 9191-9200
        timeout: 5
        auto_discover: false
        interval: 0
    scheduler:
        strategy: round_robin
        max_queue_size: 100
        task_timeout: 300
        retry_on_failure: true
        max_retries: 3
    log_aggregation:
        enabled: true
        max_buffer_size: 1048576
        flush_interval: 10
client:
    enabled: false
    master_address: ""
    client_info:
        id: ""
        name: ""
        tags: []
        metadata:
            arch: amd64
            os: linux
    heartbeat:
        interval: 30
        timeout: 90
    conda_env:
        enabled: false
        conda_path: ""
        environments:
            shepherd: ""
    register_retry: 3
    heartbeat_interval: 5
    heartbeat_timeout: 15
node:
    id: auto
    name: ""
    role: standalone
    tags: []
    metadata:
        arch: amd64
        os: linux
    master_role:
        enabled: false
        port: 9190
        api_key: ""
        ssl:
            enabled: false
            cert_path: ""
            key_path: ""
    client_role:
        enabled: false
        master_address: ""
        register_retry: 3
        heartbeat_interval: 5
        heartbeat_timeout: 15
    resources:
        monitor_interval: 5
        report_gpu: true
        report_temperature: true
        gpu_backend: auto
    executor:
        max_concurrent: 4
        task_timeout: 3600
        allow_remote_stop: true
        allowed_commands:
            - load_model
            - unload_model
            - run_llamacpp
            - stop_process
            - scan_models
            - collect_logs
    capabilities:
        python_enabled: false
        conda_path: ""
        conda_environments:
            shepherd: ""
